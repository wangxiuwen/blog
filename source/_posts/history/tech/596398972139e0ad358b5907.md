---
title: Ridge/LASSO 回归
date: 2017-07-10 11:09:11
tags: ["tech","技术","Ridge","LASSO","回归"]
author: wangxiuwen
categories: ["技术"]
layout: post
---

# Ridge/LASSO 回归

Ridge:

    岭回归


加上正则化， 也可以做，需要输入超参数，可能是个alpha值，需要人工制定，往往使用交叉验证指定
GridSearchCV 中的 CV 就是 cross vilidation。
>2016年9月之前可能是 0.17 版本的 sklearn 是                
>from sklearn.cross_validation import GridSearchCV


加上一定的平方的约束项， L1 Norm, L2 Norm

- 问题
  - 正则化是什么？
  - 超参数是什么？
  - 一般的结论是 加上 L2 正则化(radge)指标上往往是比LASSO更好的。但是LASSO 可以帮助我们得到稀疏的模型? 很多参数可以是 0, 或者是比较小的数，帮助我们做特征提取
  - alpha_can 是什么？
  - radge
  - R 方 其实指的是拟合效果， R方越大，拟合效果越好， R 方可以为负



## 完整代码

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import Lasso, Ridge
    from sklearn.model_selection import GridSearchCV

    if __name__ == "__main__":
        # pandas读入
        data = pd.read_csv('8.Advertising.csv')    # TV、Radio、Newspaper、Sales
        x = data[['TV', 'Radio', 'Newspaper']]
        # x = data[['TV', 'Radio']]
        y = data['Sales']
        print x
        print y

        x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)
        # print x_train, y_train
        model = Lasso()
        # model = Ridge()

        alpha_can = np.logspace(-3, 2, 10)
        lasso_model = GridSearchCV(model, param_grid={'alpha': alpha_can}, cv=5)
        lasso_model.fit(x, y)
        print '验证参数：\n', lasso_model.best_params_

        y_hat = lasso_model.predict(np.array(x_test))
        mse = np.average((y_hat - np.array(y_test)) ** 2)  # Mean Squared Error
        rmse = np.sqrt(mse)  # Root Mean Squared Error
        print mse, rmse

        t = np.arange(len(x_test))
        plt.plot(t, y_test, 'r-', linewidth=2, label='Test')
        plt.plot(t, y_hat, 'g-', linewidth=2, label='Predict')
        plt.legend(loc='upper right')
        plt.grid()
        plt.show()