---
title: logstash hadoop Failed to flush outgoing items AlreadyBeingCreatedException
date: 2016-06-25 10:24:40
tags: ["tech","技术","hdfs"]
author: wangxiuwen
categories: ["技术"]
layout: post
---

logstash 配置文件：[code]input {
    kafka {
        zk_connect =&gt; &quot;xxxxxxx:2181/kafka/xxx&quot;
        group_id =&gt; &quot;logstash_hadoop&quot;
        topic_id =&gt; &quot;log&quot;
        reset_beginning =&gt; true
        consumer_threads =&gt; 5
        decorate_events =&gt; true
        codec =&gt; json
    }
}
filter {
   date {
            locale =&gt; &quot;en&quot;
            match =&gt; [&quot;timestamp&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;]
        }
  
}
 webhdfs {
            use_httpfs =&gt; false
            flush_size =&gt; 5
            idle_flush_time =&gt; 0.5
            workers =&gt; 1
            host =&gt; &quot;xxx.xxx.xx.x&quot;
            port =&gt; 50070
            user =&gt; &quot;logstash&quot;
            path =&gt; &quot;/api/logs/dt=%{+YYYY-MM-dd}/logstash-%{+HH}.log&quot;
        }[/code]
 
问题 1:  Failed to flush outgoing items[code]{:timestamp=&gt;&quot;2016-06-25T09:26:37.151000+0800&quot;, :message=&gt;&quot;Failed to flush outgoing items&quot;, :outgoing_count=&gt;1, :exception=&gt;&quot;WebHDFS::ServerError&quot;, :backtrace=&gt;[&quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/webhdfs-0.8.0/lib/webhdfs/client_v1.rb:351:in `request'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/webhdfs-0.8.0/lib/webhdfs/client_v1.rb:349:in `request'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/webhdfs-0.8.0/lib/webhdfs/client_v1.rb:270:in `operate_requests'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/webhdfs-0.8.0/lib/webhdfs/client_v1.rb:73:in `create'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-2.0.4/lib/logstash/outputs/webhdfs.rb:210:in `write_data'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-2.0.4/lib/logstash/outputs/webhdfs.rb:205:in `write_data'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-2.0.4/lib/logstash/outputs/webhdfs.rb:195:in `flush'&quot;, &quot;org/jruby/RubyHash.java:1342:in `each'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-2.0.4/lib/logstash/outputs/webhdfs.rb:183:in `flush'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/buffer.rb:219:in `buffer_flush'&quot;, &quot;org/jruby/RubyHash.java:1342:in `each'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/buffer.rb:216:in `buffer_flush'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/buffer.rb:159:in `buffer_receive'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-2.0.4/lib/logstash/outputs/webhdfs.rb:166:in `receive'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-core-2.0.0-java/lib/logstash/outputs/base.rb:80:in `handle'&quot;, &quot;(eval):409:in `output_func'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-core-2.0.0-java/lib/logstash/pipeline.rb:252:in `outputworker'&quot;, &quot;/usr/local/logstash-2.0.0/vendor/bundle/jruby/1.9/gems/logstash-core-2.0.0-java/lib/logstash/pipeline.rb:169:in `start_outputs'&quot;], :level=&gt;:warn}

[/code]
解决方案：
1. 需要在 logstash 所在机器 /etc/hosts 配置 hadoop 所在机器，
2. logstash 配置的用户要对 hdfs 目录有写权限 (root 用户则不用 chown chgrp)
[code]hdfs dfs -mkdir -p /api/logs/
hdfs dfs -chown logstash /api/logs/
hdfs dfs -chgrp logstash /api/logs/[/code]
 
 
 问题 2:   AlreadyBeingCreatedException
 
原因：
1台机器，数据备份默认设置为3，通过webhdfs写报错。修改备份数为1正常。
[quote]
hdfs-site.conf
[/quote]
 [code]&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;master:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/data/hadoop/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/data/hadoop/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.support.append&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.support.broken.append&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;[/code]
 
 