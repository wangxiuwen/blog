---
title: spark MongoDB 读取大集合文档 schema缺失字段
date: 2018-11-09 08:54:58
tags: ["tech","技术"]
author: baipeng
categories: ["技术"]
layout: post
---

```
场景：使用Spark 读取MongoDB中的集合数据，由于文档比较大。mongo-spark-connector  插件默认选择 1000条了推断schema 导致缺失字段。

```
![image.png](/images/59d092356e0c2241629235eec5de7e0f.png)


```
解决办法1：最懒的办法。先查询出来需要读取的数据count,然后在查一下设置sampleSize的大小。可以搞定
```
```
解决办法2：获取要查询的字段的list，自己定义schema
       var schemaString = Joiner.on(" ").join(lists)
       println(schemaString)
       //    定义schema
        val schema =
         StructType(schemaString.split(" ").map(fieldName => StructField(fieldName.split(":")(0), if (fieldName.split(":")(1).equals("String")) StringType else IntegerType, true)))
       val df = spark.createDataFrame(d.rdd, schema)
```
```
解决办法3：定义结构类
  	case class Creature(name: String, strength: Int, type: String)
	val explicitDF = MongoSpark.load[Creature](sparkSession)()
	explicitDF.printSchema()
```