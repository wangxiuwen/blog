---
title: spark 从下载到 hello world
date: 2016-05-28 12:48:48
tags: ["tech","技术","spark"]
author: wangxiuwen
categories: ["技术"]
layout: post
---

Spark 采用的是 master/slave 架构。Spark 可以运行在yarn mesos 等资源管理框架之上， 也可以以 Standalone cluster manager 进行集群资源管理。
Spark 依赖 Hadoop HDFS 作为数据存储。如果是纯计算集群，可以不依赖Hadoop。Spark本身的启动运行是不依赖Hadoop的。
 
Spark集群可以以下方式运行：
[quote]
主节点 (Spark master node 和 Hadoop NameNode)
从节点 (Spark worker nodes 和 Hadoop DataNodes) 
客户端 (Driver node)
[/quote]
用户的 driver program 运行在 driver node上。
 
以下是Spark 架构图：

 ![41fe6e1c96959fbcb9bc4121b05281e0.png](/images/30245061effef34a0f279e8c5391f4e3.png)

 
 
以 ubuntu 为例：[code]root@qianrushi:/usr/local/spark-1.6.0-bin-hadoop2.6/conf# cat /etc/issue
Ubuntu 14.04.4 LTS \n \l[/code]
 首先我们需要配置主机名称，这需要配置两个文件：
[quote]
/etc/hosts
[/quote]
[quote]
/etc/hostname
[/quote]
[code]root@qianrushi:/usr/local/spark-1.6.0-bin-hadoop2.6/conf# cat /etc/hostname
qianrushi
root@qianrushi:/usr/local/spark-1.6.0-bin-hadoop2.6/conf# cat /etc/hosts
#127.0.0.1 localhost
192.168.200.111 localhost
192.168.200.111 qianrushi
#127.0.1.1    localhost.localdomain    localhost
192.168.200.111    localhost.localdomain    localhost[/code] [code]root@ubuntu-server:~# tail -n 10 /etc/profile
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH

export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin

export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin

[/code][code]//----------------------------------------------------------------
core-site.xml

&lt;configuration&gt;
   &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://ubuntu-server:9011/&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
         &lt;value&gt;file:/data/hadoop/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

//----------------------------------------------------------------
hadoop-env.sh 

export JAVA_HOME=/usr/local/java/jdk1.8.0_91
//----------------------------------------------------------------
hdfs-site.xml

&lt;configuration&gt;
   &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;ubuntu-server:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/data/hadoop/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/data/hadoop/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;


将Hadoop.tmp.dir所指定的目录删除。

重新执行命令：hadoop namenode -format

只有namenode需要format，secondarynamenode和datanode不需要format。







[/code]


 可以通过spark-shell做简单测试：
[quote]
SPARK_EXECUTOR_MEMORY 的大小不能超过节点可用内存
[/quote]
[code]SPARK_EXECUTOR_MEMORY=2g ./bin/spark-shell --master spark://master:7077[/code]
测试1:[code]scala&gt;val data = new Array[Int](1000000)
scala&gt;for (i &lt;- 0 until data.length)
      | data(i) = i + 1
scala&gt;val distData = sc.parallelize(data)
scala&gt;distData.reduce(_+_)[/code]
 
测试2:
 
先将文件传到hdfs上：[code]root@qianrushi:~# cat test.txt
a
b
c
d
root@qianrushi:~# hdfs dfs -ls /
root@qianrushi:~# hdfs dfs -put test.txt /[/code]

进入sparks-shell：[code]scala&gt;val textFile = sc.textFile(&quot;hdfs://master:9000/test.txt&quot;)
scala&gt;textFile.filter(line =&gt; line.contains(&quot;a&quot;)).count()[/code]
 spark官方文档测试 sparksql：[code]scala&gt;val sqlContext = new org.apache.spark.sql.SQLContext(sc)
scala&gt;val df = sqlContext.read.json(&quot;examples/src/main/resources/people.json&quot;)
scala&gt;df.show()
scala&gt;df.printSchema()
scala&gt;df.select(&quot;name&quot;).show()[/code]
 
 
默认端口：
[quote]
Hadoop NameNode 50070    http://&lt;Hadoop NameNode private IP&gt;:50070
Spark master 8080  http://&lt;spark master private IP&gt;:8080
[/quote]

 
 cankao:
[quote]
http://www.cnblogs.com/ringwang/p/3623149.html
http://hsrong.iteye.com/blog/1374734
http://www.ubuntukylin.com/ukylin/forum.php?mod=viewthread&amp;tid=4702

http://blog.csdn.net/yeruby/article/details/21542465
[/quote]